{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19QHr3lQ9Tl9lp7ylmul8JKs-VFw3KMsT","timestamp":1698952250392}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7aHmP5vgqd4j"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install 'typing-extensions<4.6.0' --force-reinstall\n","!pip install cryptocompare\n","!pip install alpaca-py"],"metadata":{"id":"TEwfFU4Ksabl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import datetime as datetime\n","import cryptocompare\n","import csv\n","import numpy as np\n","import random\n","from collections import deque\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential, load_model\n","from alpaca.trading.client import TradingClient\n","from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, Attention\n","from alpaca.trading.requests import MarketOrderRequest\n","from alpaca.trading.enums import OrderSide, TimeInForce"],"metadata":{"id":"_qnKU9WaPIjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cryptocompare.cryptocompare._set_api_key_parameter(\"8632283f7e59e6054da95d4fc2728404b2931eee776abce0d69d86af35a86e54\")\n","\n","#The limit to request is 999 hours\n","data = cryptocompare.get_historical_price_hour('ETH', 'USD', limit=999, exchange='CCCAGG', toTs=datetime.datetime.now())\n","\n","#Removing unwanted values\n","headers_to_exclude = ['conversionType', 'conversionSymbol']\n","cleaned_data = [{k: v for k, v in item.items() if k not in headers_to_exclude} for item in data]\n","\n","csv_file = \"/content/eth_1_year_data.csv\"\n","\n","#Writing data to the csv\n","with open(csv_file, 'w', newline='') as file:\n","    writer = csv.DictWriter(file, fieldnames=cleaned_data[0].keys())\n","    writer.writeheader()\n","    writer.writerows(cleaned_data)\n","file.close()\n","\n","#Using data from the CSV to calculate sma and adding it to the csv\n","df = pd.read_csv('/content/eth_1_year_data.csv')\n","df['50sma'] = df['close'].rolling(50).mean()\n","df['20sma'] = df['close'].rolling(20).mean()\n","\n","# Drop rows with NaN values after calculating SMAs\n","df.dropna(inplace=True)\n","\n","df.to_csv(csv_file)"],"metadata":{"id":"koV1id6dhSKu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data\n","csv_file = \"/content/eth_1_year_data.csv\"\n","df = pd.read_csv(csv_file)\n","\n","# Assuming 'close' is the feature we want to predict\n","# Let's also include 'high', 'low', 'open', 'volumefrom', 'volumeto', '20sma', and '50sma' as features\n","features = ['high', 'low', 'open', 'volumefrom', 'volumeto', 'close', '20sma', '50sma']\n","df[features] = df[features].fillna(method='ffill')  # Forward fill for missing values\n","\n","# Normalize the data\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(df[features].values)\n","\n","# Function to create sequences\n","def create_sequences(data, time_steps=20):\n","    X, y = [], []\n","    for i in range(time_steps, len(data)):\n","        X.append(data[i-time_steps:i])\n","        y.append(data[i, 0])  # Assuming 'close' price is at index 5\n","    return np.array(X), np.array(y)\n","\n","# Create sequences\n","time_steps = 20  # Number of time steps you're looking back to predict the future\n","X, y = create_sequences(scaled_data, time_steps)\n","\n","# Split the data\n","split = int(0.8 * len(X))  # 80% for training and 20% for testing\n","X_train, X_test = X[:split], X[split:]\n","y_train, y_test = y[:split], y[split:]\n","\n","# LSTM expects input to be in the shape of [samples, time steps, features]\n","input_shape = (X_train.shape[1], X_train.shape[2])\n","\n","# Define the LSTM model\n","def lstm_model(input_shape):\n","    inputs = Input(shape=input_shape)\n","    lstm_out, hidden_state, cell_state = LSTM(50, activation='relu', return_sequences=True, return_state=True)(inputs)\n","    lstm_out = Dropout(0.2)(lstm_out)\n","    query_value_attention_seq = Attention()([lstm_out, lstm_out])\n","    query_value_attention_concat = Concatenate(axis=-1)([lstm_out, query_value_attention_seq])\n","    lstm_out_2 = LSTM(50, activation='relu', return_sequences=False)(query_value_attention_concat)\n","    lstm_out_2 = Dropout(0.2)(lstm_out_2)\n","    output = Dense(1, activation='linear')(lstm_out_2)\n","    model = Model(inputs=inputs, outputs=output)\n","    model.compile(optimizer=Adam(), loss='mean_squared_error')\n","    return model\n","\n","# Build the model\n","attention_lstm_model = lstm_model(input_shape)\n","\n","# Train the model\n","history = attention_lstm_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n","\n","# Save the trained LSTM model\n","attention_lstm_model.save('attention_lstm_model.h5')\n","\n","# Evaluate the model\n","loss = attention_lstm_model.evaluate(X_test, y_test, verbose=0)\n","print(f'Test loss: {loss}')"],"metadata":{"id":"WthNhscJKKEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# After training, use the most recent data to make a prediction\n","last_sequence = X_test[-1].reshape((1, time_steps, X_test.shape[2]))\n","predicted_price_scaled = attention_lstm_model.predict(last_sequence)\n","\n","# Invert the scaling for the predicted price\n","predicted_price = scaler.inverse_transform(np.concatenate((predicted_price_scaled, np.zeros((predicted_price_scaled.shape[0], len(features)-1))), axis=1))[:,0]\n","\n","# Fetch the current price of ETH from CryptoCompare\n","current_price_data = cryptocompare.get_price('ETH', currency='USD')\n","current_price = current_price_data['ETH']['USD']\n","\n","# Calculate the prediction error\n","percentage_error = abs(current_price - predicted_price) / current_price * 100\n","\n","# Output the results\n","print(f\"Predicted ETH Price: {predicted_price[0]} USD\")\n","print(f\"Current ETH Price: {current_price} USD\")\n","print(f\"Prediction Error: {percentage_error[0]:.2f}%\")"],"metadata":{"id":"MHcjnKXLagSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize your API keys (these should be kept secret and not hardcoded in production)\n","trading_client = TradingClient('PK7LPFB3RFNB9XC9C9SR', 'qxtASZdlaDRJDDkQINDlRSVhsRSwPDBUO6L9bnG3')"],"metadata":{"id":"NZKbkkKbqD8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MarketEnvironment:\n","    def __init__(self, csv_file_path, initial_balance=100000, lookback_window_size=20):\n","        # Load historical data from CSV file\n","        self.data = pd.read_csv(\"/content/eth_1_year_data.csv\")\n","\n","        # Initialize environment parameters\n","        self.initial_balance = initial_balance\n","        self.lookback_window_size = lookback_window_size\n","\n","        # Determine state size\n","        self.state_size = (lookback_window_size, self.data.shape[1] + 2)\n","\n","        # Initialize variables to keep track of state\n","        self.balance = initial_balance\n","        self.holdings = 0\n","        self.total_portfolio_value = self.balance\n","        self.done = False\n","        self.current_step = 0\n","\n","        # Initialize any trading client or API if needed\n","        self.alpaca_api = trading_client\n","\n","    def reset(self):\n","        # Reset environment state\n","        self.balance = self.initial_balance\n","        self.holdings = 0\n","        self.total_portfolio_value = self.balance\n","        self.current_step = 0\n","        self.done = False\n","        return self.get_state(self.current_step)\n","\n","    def step(self, action):\n","        # Validate action\n","        assert action in [0, 1, 2]\n","\n","        # Fetch current market data and update balance/holdings from Alpaca\n","        self._update_portfolio()\n","\n","        current_price = self._get_current_price()\n","        reward = 0\n","\n","        fixed_qty_to_trade = 1\n","\n","        if action == 1:  # Buy\n","            # Implement buy action\n","            reward = self._buy(current_price, fixed_qty_to_trade)\n","\n","        elif action == 2 and self.holdings > 0:  # Sell\n","            # Implement sell action\n","            qty_to_sell = min(self.holdings, fixed_qty_to_trade)\n","            reward = self._sell(current_price, qty_to_sell)\n","\n","        # Update the state and check if done\n","        self.current_step += 1\n","        if self.current_step >= len(self.data):\n","            self.done = True\n","\n","        next_state = self.get_state(self.current_step)\n","        return next_state, reward, self.done, {}\n","\n","    def _update_portfolio(self):\n","        # Fetch the latest account info and update balance and holdings\n","        # Implement the logic based on your actual portfolio on Alpaca\n","        self.balance = float(self.alpaca_api.get_account().cash)\n","        # self.holdings needs to be updated based on the current holdings in Alpaca\n","\n","    def _get_current_price(self):\n","        # Fetch the current market price from CryptoCompare\n","        return cryptocompare.get_price('ETH', currency='USD')['ETH']['USD']\n","\n","    def _buy(self, current_price, qty):\n","        try:\n","            # Creating MarketOrderRequest object for buy order\n","            market_order_data = MarketOrderRequest(\n","            symbol=\"ETHUSD\",\n","            qty=1,\n","            side=OrderSide.BUY,\n","            time_in_force=TimeInForce.GTC  # Good Till Cancelled\n","            )\n","\n","            # Submit the market order\n","            market_order = self.alpaca_api.submit_order(order_data=market_order_data)\n","            print(\"Buy order submitted successfully:\", market_order)\n","        except Exception as e:\n","            print(f\"An error occurred while submitting the buy order: {e}\")\n","\n","            # Update balance and holdings\n","            self.balance -= 1 * current_price\n","            self.holdings += 1\n","            return -1\n","\n","    def _sell(self, current_price, qty):\n","        try:\n","            # Creating MarketOrderRequest object for sell order\n","            market_order_data = MarketOrderRequest(\n","                symbol=\"ETHUSD\",\n","                qty=1,\n","                side=OrderSide.SELL,\n","                time_in_force=TimeInForce.GTC\n","            )\n","\n","            # Submitting market order\n","            market_order = self.alpaca_api.submit_order(order_data=market_order_data)\n","            print(\"Sell order submitted successfully:\", market_order)\n","        except Exception as e:\n","            print(f\"An error occurred while submitting the sell order: {e}\")\n","\n","        # Update balance and holdings\n","        self.balance += self.holdings * current_price\n","        reward = self.balance - self.initial_balance\n","        self.holdings = 0\n","        return reward\n","\n","    def send_test_order(self, symbol, qty, side, order_type, time_in_force):\n","        print(f\"Sending a {side} order for {qty} shares of {symbol}...\")\n","        try:\n","            if \"paper-api\" in self.alpaca_api._base_url:\n","                order = self.alpaca_api.submit_order(\n","                    symbol=symbol,\n","                    qty=qty,\n","                    side=side,\n","                    type=order_type,\n","                    time_in_force=time_in_force\n","                )\n","                print(\"Order submitted successfully!\")\n","                print(f\"Order details: ID={order.id}, Status={order.status}\")\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","\n","    def get_state(self, current_step):\n","        # Determine the window of data to consider.\n","        window_start = max(current_step - self.lookback_window_size, 0)\n","        window_end = current_step + 1\n","        window_data = self.data.iloc[window_start:window_end].values  # Use .iloc for DataFrame\n","\n","        # Debug statement to print shapes\n","        print(f\"window_data shape: {window_data.shape}\")\n","\n","        # Create a 2D array for balance and holdings, repeat it to match the window_data's first dimension\n","        balance_and_holdings = np.array([[self.balance, self.holdings]] * window_data.shape[0])\n","\n","        # Debug statement to print shapes\n","        print(f\"balance_and_holdings shape: {balance_and_holdings.shape}\")\n","\n","        # Concatenate along the second axis (axis=1) to include balance and holdings in the state\n","        state = np.concatenate((window_data, balance_and_holdings), axis=1)\n","\n","        # Debug statement to print final state shape\n","        print(f\"state shape: {state.shape}\")\n","\n","        return state"],"metadata":{"id":"bJLdHhmTlDQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TradingAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=2000)  # Memory for experience replay\n","        self.gamma = 0.95  # Discount rate for future rewards\n","        self.epsilon = 1.0  # Exploration rate\n","        self.epsilon_min = 0.01  # Minimum exploration rate\n","        self.epsilon_decay = 0.995  # Decay rate for exploration probability\n","        self.learning_rate = 0.001  # Learning rate\n","        self.model = self._build_model()  # The DQN model\n","        self.lstm_model = load_model('attention_lstm_model.h5')\n","\n","    def _build_model(self):\n","        \"\"\"Builds a NN model.\"\"\"\n","        model = Sequential()\n","        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n","        model.add(Dense(24, activation='relu'))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        \"\"\"Stores experiences in memory.\"\"\"\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        \"\"\"Selects an action based on the current state.\"\"\"\n","        if random.uniform(0, 1) <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        act_values = self.model.predict(state)\n","        return np.argmax(act_values[0])  # Returns the action with the highest Q-value\n","\n","    def replay(self, batch_size):\n","        \"\"\"Trains the agent by replaying experiences from memory.\"\"\"\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n","            target_f = self.model.predict(state)\n","            target_f[0][action] = target\n","            self.model.fit(state, target_f, epochs=1, verbose=0)\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","    def load(self, name):\n","        \"\"\"Loads a saved model.\"\"\"\n","        self.model.load_weights(name)\n","\n","    def save(self, name):\n","        \"\"\"Saves the model.\"\"\"\n","        self.model.save_weights(name)"],"metadata":{"id":"XcA2VC7qmVsf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MAIN LOOP\n","\n","# Initialize the market environment\n","initial_balance = 100000  # Set an initial balance, for example, $100,000\n","lookback_window_size = 1  # The number of past time points to consider for the state\n","\n","# Initialize the market environment\n","market_environment = MarketEnvironment(data, initial_balance, lookback_window_size)\n","\n","# Define the size of the state and action space\n","state_size = market_environment.state_size[0] * market_environment.state_size[1]\n","action_size = 3  # Assuming three actions: [hold, buy, sell]\n","\n","# Initialize the trading agent\n","trading_agent = TradingAgent(state_size, action_size)\n","\n","# How many episodes we want the agent to train for\n","num_episodes = 150 # Tweak according to performance\n","\n","# How many timesteps per episode\n","timesteps_per_episode = len(data) - lookback_window_size\n","print(f\"timesteps_per_episode: {timesteps_per_episode}\") # Currently 999 hours or 41.625 days of historical data\n","\n","# Batch size for experience replay\n","batch_size = 32 # 32 is a commonly used default because it's been found to be a good trade-off between training speed and model update stability\n","\n","for e in range(num_episodes):\n","    # Reset the market environment at the start of each episode\n","    state = market_environment.reset()\n","\n","    # Flatten the state to feed into the neural network\n","    state = np.reshape(state, (1, -1))  # This will ensure the state is a 2D array with the correct shape\n","\n","    for time in range(timesteps_per_episode):\n","        # The agent takes action\n","        action = trading_agent.act(state)\n","\n","        # Apply the action to the market environment to get the next state and reward\n","        next_state, reward, done, _ = market_environment.step(action)\n","\n","        # Remember the previous state, action, reward, and next state\n","        trading_agent.remember(state, action, reward, next_state, done)\n","\n","        # Make the next_state the new current state for the next frame.\n","        state = next_state\n","\n","        # If done, exit the loop\n","        if done:\n","            print(f\"Episode: {e + 1}/{num_episodes}, Total Portfolio Value: {market_environment.total_portfolio_value}, P/L: {market_environment.total_portfolio_value - initial_balance}\")\n","            break\n","\n","        # Train the agent with experiences in replay memory\n","        if len(trading_agent.memory) > batch_size:\n","            trading_agent.replay(batch_size)\n","\n","    # Optionally, you can save the model every X episodes\n","    if e % 10 == 0:\n","        trading_agent.save(f\"trading_agent_{e}.h5\")\n","\n","# Testing the buy/sell functions here\n","# Initializing our API keys\n","trading_client = TradingClient('PK7LPFB3RFNB9XC9C9SR', 'qxtASZdlaDRJDDkQINDlRSVhsRSwPDBUO6L9bnG3', base_url='https://paper-api.alpaca.markets')\n","\n","# Ensuring the market_environment is using the trading_client instance with our API keys\n","market_environment.alpaca_api = trading_client\n","\n","# Example usage\n","csv_file_path = '/content/eth_1_year_data.csv'\n","market_environment = MarketEnvironment(csv_file_path, initial_balance=200000, lookback_window_size=20)\n","market_environment.send_test_order(\n","    symbol='ETHUSD',\n","    qty=1,\n","    side='buy',\n","    order_type='market',\n","    time_in_force='gtc'\n",")\n","market_environment.send_test_order(\n","    symbol='ETHUSD',\n","    qty=1,\n","    side='sell',\n","    order_type='market',\n","    time_in_force='gtc'\n",")"],"metadata":{"id":"YdddtiPNn9KI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"_CuFBiMWTDbk"}},{"cell_type":"code","source":["\"\"\"\n","MODEL TIMING\n","\n","The total time per episode in seconds would be 0.024 seconds * 999 timesteps = 23.976 seconds.\n","\n","For 150 episodes, the total time would be 23.976 seconds/episode * 150 episodes = 3596.4 seconds.\n","\n","Converting this to hours: 3596.4 seconds / 3600 seconds/hour ≈ 0.999 hours.\n","\"\"\""],"metadata":{"id":"VNevvhxFOpRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","HYPERPARAMETERS\n","\n","Time Steps (time_steps): Used in the sequence creation function, it defines the number of historical data points the LSTM model will consider for predicting the next value. It's set to 20, meaning the model looks at the last 20 data points to make a prediction.\n","\n","Data Split (split): Determines the ratio of training to testing data. It's set to 80% for training and the remaining 20% for testing.\n","\n","LSTM Units: Inside the lstm_model function, LSTM layers with 50 units are defined, which signifies the dimensionality of the output space.\n","\n","Dropout Rate: Dropout layers with a rate of 0.2 are used to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training.\n","\n","Dense Layer Units: The dense layers in the LSTM model have 24 units each, which is another hyperparameter defining the output dimension of the layer.\n","\n","Batch Size for Training LSTM Model (batch_size): The LSTM model is trained with a batch size of 32, which means 32 sequences are passed through the network at once.\n","\n","Epochs for Training LSTM Model: The model is trained for 100 epochs, where one epoch means that the entire dataset is passed forward and backward through the neural network once.\n","\n","Lookback Window Size (lookback_window_size): Set to 20, this defines how many past observations the trading environment will return as the state.\n","\n","State Size (state_size): Calculated based on the lookback_window_size and the number of features in the data. It represents the shape of the input that the trading agent will receive.\n","\n","Gamma (gamma): This is the discount factor for future rewards in the DQN agent, set to 0.95.\n","\n","Epsilon (epsilon): The exploration rate for the agent, starting at 1.0 (100% exploration).\n","\n","Epsilon Minimum (epsilon_min): The minimum exploration rate, set to 0.01.\n","\n","Epsilon Decay (epsilon_decay): The rate at which the exploration rate decays, set to 0.995.\n","\n","Learning Rate (learning_rate): For the Adam optimizer in the DQN, it's set to 0.001.\n","\n","Number of Episodes (num_episodes): The number of episodes for training the DQN model. It's initially set to 1800, which indicates how many times the training process will iterate over the entire dataset.\n","\n","Timesteps Per Episode (timesteps_per_episode): The number of steps the agent will take in the environment per episode. It's initially logged as 999, which is derived from the available historical price hours minus the lookback_window_size.\n","\"\"\""],"metadata":{"id":"7tO_hf1TXKhm"},"execution_count":null,"outputs":[]}]}